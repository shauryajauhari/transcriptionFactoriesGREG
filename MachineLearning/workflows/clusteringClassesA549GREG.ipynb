{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the Classes | Ascertaining Key Variables in the Hubs and Non-Hubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"justify\"> This workflow is premised on the idea that with the clustering of the data on read-coverages (for the features), we'll be able to figure out the key variables that are truly representative of the Hubs or Non- Hubs, as defined by our previous analyses. </p>\n",
    "<p align = \"justify\"> So, we began with a data matrix that held information about the reads associated with histone marks and protein-bindings, corresponding to our bin-intervals of 2Kb (genome-wide). From an initial analysis, we were able to classifiy such intervals as \"Hubs\" or \"Non-Hubs\". <i> Hubs </i> are those regions that interact in the distance of 1Mb or \"Inf\", as per the definitions in <a href= \"https://doi.org/10.1093/database/baz162\" > GREG </a>. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: vector memory exhausted (limit reached?)\n",
     "output_type": "error",
     "traceback": [
      "Error: vector memory exhausted (limit reached?)\nTraceback:\n",
      "1. dist(one[, -c(1:5, 18)], method = \"euclidean\")"
     ]
    }
   ],
   "source": [
    "# --- Loading Data --- #\n",
    "\n",
    "classData <- read.table(\"../results/truePositives/a549LR.txt\", header = T)\n",
    "featureData <- read.table(\"../data/A549forML.txt\", header = T)\n",
    "\n",
    "# --- Removing Redundancy --- #\n",
    "\n",
    "classDataUnique <- unique(classData)\n",
    "featureDataUnique <- unique(featureData)\n",
    "\n",
    "# --- Merging Data --- #\n",
    "\n",
    "library(dplyr)\n",
    "one <- inner_join(classDataUnique, featureDataUnique, by = c(\"chr\", \"start\", \"end\"))\n",
    "\n",
    "# --- Applying Hierarchical Clustering --- #\n",
    "\n",
    "distance <- dist(one[, - c(1:5, 18)], method = \"euclidean\")\n",
    "cluster <- hclust(distance, method = \"average\")\n",
    "plot(cluster, hang = -1, label = one$Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical clustering is sensitive to the extent of data, and didn't execute successfully. Let us try a different flavor; k-means clustering. But before that, let's look at the distribution of variables which holds clue to nature of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
